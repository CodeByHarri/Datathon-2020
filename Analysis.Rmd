---
title: <center> <h1>Can electricity consumption patterns tell us anything about the pandemic?</h1> </center>
author: 
output:
  html_document: 
    toc: false
    theme: flatly
    toc_float: true
    collapsed: true
    number_sections: false
    toc_depth: 1
    # code_folding: hide
editor_options: 
  chunk_output_type: inline
---

<center>
*This report explores electricity consumption patterns within forecasting, demand and behaviours to tackle if data can show us actionable insights.*
</center><br> <!-- New line --> 

1. **Forecasting**: *Forecasting 2019 vs 2020 using SARIMAX & intervential anlaysis*

2. **Demand & Economics**: *Duck curve & the economy - An analysis into electricity consumption*

3. **Anomalous Behaviours**: *An experimental approach to detect change in production activity*

</center>


things left to do: 

* data wrangle australian, and american data into the same format as euro data. 
* remake the apps to acconomadate for extra data. 
* pretrains and save models for all combinations of countrys and years of forecast
* make an australia only data that by 5min to apply befords law




<center> <h2>**Forecasting**: Forecasting 2019 vs 2020 using SARIMAX & intervential anlaysis</h2> </center>

work in progress
```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(dplyr)
library(plotly)
library(itertools)
library(scales)
library(forecast)
```



```{r 1, echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}


# # WESTERN EURO DATA DAY
# 
# fr = read.csv("eurodata/fr.csv", header =TRUE, stringsAsFactors = FALSE)
# fr$end = fr$end %>% as.Date()
# fr = aggregate(fr["load"], by=fr["end"], sum)
# fr = fr[1:(length(fr$load)-1),]
# df = fr
# setwd("eurodata/")
# temp = list.files(pattern="*.csv")
# for (i in 1:length(temp)){
#   fr = read.csv(temp[i], header = TRUE, stringsAsFactors = FALSE)
#   fr = na.omit(fr)
#   fr$end = fr$end %>% as.Date()
#   fr = aggregate(fr["load"], by=fr["end"], sum)
#   fr = fr[1:(length(fr$load)-1),]
#   fr[,paste0(temp[i])] = fr$load
#   df = left_join(df, fr, by=c("end"))
# }
# df = df[, -grep("load", colnames(df))]
# colnames(df) = c('end', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden')
# 
# 
# 
# # AUSTRALIA DATA
# 
# aus = read.csv("ausdata/VIC/PRICE_AND_DEMAND_201501_VIC1.csv", header =TRUE, stringsAsFactors = FALSE)
# aus$SETTLEMENTDATE = aus$SETTLEMENTDATE %>% as.Date()
# aus = aggregate(aus["TOTALDEMAND"], by=aus["SETTLEMENTDATE"], sum)
# aus = aus %>% filter(TOTALDEMAND > 10000)
# 
# temp = list.files("ausdata/VIC/",pattern="*.csv")
# for (i in 1:length(temp)){
#   ausx = read.csv(paste0("ausdata/VIC/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   ausx$SETTLEMENTDATE = ausx$SETTLEMENTDATE %>% as.Date()
#   ausx = aggregate(ausx["TOTALDEMAND"], by=ausx["SETTLEMENTDATE"], sum)
#   ausx = ausx %>% filter(TOTALDEMAND >= 10000)
#   aus = rbind(aus, ausx)
# }
# 
# VIC = aus[!duplicated(aus), ]
# 
# aus = read.csv("ausdata/NSW/PRICE_AND_DEMAND_201501_NSW1.csv", header =TRUE, stringsAsFactors = FALSE)
# aus$SETTLEMENTDATE = aus$SETTLEMENTDATE %>% as.Date()
# aus = aggregate(aus["TOTALDEMAND"], by=aus["SETTLEMENTDATE"], sum)
# aus = aus %>% filter(TOTALDEMAND > 10000)
# 
# temp = list.files("ausdata/NSW/",pattern="*.csv")
# for (i in 1:length(temp)){
#   ausx = read.csv(paste0("ausdata/NSW/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   ausx$SETTLEMENTDATE = ausx$SETTLEMENTDATE %>% as.Date()
#   ausx = aggregate(ausx["TOTALDEMAND"], by=ausx["SETTLEMENTDATE"], sum)
#   ausx = ausx %>% filter(TOTALDEMAND >= 10000)
#   aus = rbind(aus, ausx)
# }
# NSW = aus[!duplicated(aus), ]
# 
# 
# aus = read.csv("ausdata/QLD/PRICE_AND_DEMAND_201501_QLD1.csv", header =TRUE, stringsAsFactors = FALSE)
# aus$SETTLEMENTDATE = aus$SETTLEMENTDATE %>% as.Date()
# aus = aggregate(aus["TOTALDEMAND"], by=aus["SETTLEMENTDATE"], sum)
# aus = aus %>% filter(TOTALDEMAND > 10000)
# 
# temp = list.files("ausdata/QLD/",pattern="*.csv")
# for (i in 1:length(temp)){
#   ausx = read.csv(paste0("ausdata/QLD/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   ausx$SETTLEMENTDATE = ausx$SETTLEMENTDATE %>% as.Date()
#   ausx = aggregate(ausx["TOTALDEMAND"], by=ausx["SETTLEMENTDATE"], sum)
#   ausx = ausx %>% filter(TOTALDEMAND >= 10000)
#   aus = rbind(aus, ausx)
# }
# QLD = aus[!duplicated(aus), ]
# 
# 
# aus = read.csv("ausdata/SA/PRICE_AND_DEMAND_201501_SA1.csv", header =TRUE, stringsAsFactors = FALSE)
# aus$SETTLEMENTDATE = aus$SETTLEMENTDATE %>% as.Date()
# aus = aggregate(aus["TOTALDEMAND"], by=aus["SETTLEMENTDATE"], sum)
# aus = aus %>% filter(TOTALDEMAND > 10000)
# 
# temp = list.files("ausdata/SA/",pattern="*.csv")
# for (i in 1:length(temp)){
#   ausx = read.csv(paste0("ausdata/SA/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   ausx$SETTLEMENTDATE = ausx$SETTLEMENTDATE %>% as.Date()
#   ausx = aggregate(ausx["TOTALDEMAND"], by=ausx["SETTLEMENTDATE"], sum)
#   ausx = ausx %>% filter(TOTALDEMAND >= 10000)
#   aus = rbind(aus, ausx)
# }
# SA = aus[!duplicated(aus), ]
# 
# 
# aus = read.csv("ausdata/TAS/PRICE_AND_DEMAND_201501_TAS1.csv", header =TRUE, stringsAsFactors = FALSE)
# aus$SETTLEMENTDATE = aus$SETTLEMENTDATE %>% as.Date()
# aus = aggregate(aus["TOTALDEMAND"], by=aus["SETTLEMENTDATE"], sum)
# aus = aus %>% filter(TOTALDEMAND > 10000)
# 
# temp = list.files("ausdata/TAS/",pattern="*.csv")
# for (i in 1:length(temp)){
#   ausx = read.csv(paste0("ausdata/TAS/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   ausx$SETTLEMENTDATE = ausx$SETTLEMENTDATE %>% as.Date()
#   ausx = aggregate(ausx["TOTALDEMAND"], by=ausx["SETTLEMENTDATE"], sum)
#   ausx = ausx %>% filter(TOTALDEMAND >= 10000)
#   aus = rbind(aus, ausx)
# }
# TAS = aus[!duplicated(aus), ]
# 
# remove(aus)
# aus = VIC
# aus$NSW = NSW$TOTALDEMAND
# aus$VIC = VIC$TOTALDEMAND
# aus$TAS = TAS$TOTALDEMAND
# aus$QLD = QLD$TOTALDEMAND
# aus$SA = SA$TOTALDEMAND
# 
# 
# aus = aus[, -grep("TOTALDEMAND", colnames(aus))]
# auswide = gather(aus, state, load, NSW:SA, factor_key=TRUE)
# auswide = auswide[order(auswide$SETTLEMENTDATE), ]
# #saving a state dataframe
# statewide = auswide
# 
# 
# # US DATA
# us = read.csv("usdata/EIA930_BALANCE_2015_Jul_Dec.csv", header =TRUE, stringsAsFactors = FALSE)
# us$Data.Date = us$Data.Date %>% as.Date(format = "%m/%d/%Y")
# us$Demand..MW.=  us$Demand..MW. %>% as.numeric()
# us = aggregate(Demand..MW. ~ Data.Date, data = us, sum)
# 
# temp = list.files("usdata/",pattern="*.csv")
# for (i in 1:length(temp)){
#   usx = read.csv(paste0("usdata/",temp[i]), header = TRUE, stringsAsFactors = FALSE)
#   usx$Data.Date = usx$Data.Date %>% as.Date(format = "%m/%d/%Y")
#   usx$Demand..MW.=  usx$Demand..MW. %>% as.numeric()
#   usx = aggregate(Demand..MW. ~ Data.Date, data = usx, sum)
#   us = rbind(us, usx)
# }
# US = us[!duplicated(us), ]
# US = US[order(US$Data.Date), ]
# colnames(US) = c("end","US")
# 
# # MASTER DATA FRAME
# worlddata = df[-17]
# auswide = aggregate(load ~ SETTLEMENTDATE, data = auswide, sum)
# auswide = auswide[(auswide$SETTLEMENTDATE> "2014-12-31" & auswide$SETTLEMENTDATE< "2020-08-01"),]
# worlddata = add_column(worlddata, Australia = auswide$load, .after = 1) #adds australia
# worlddata = left_join(worlddata, US, by=c("end")) # adds the US 
# 
# 
# write.csv(worlddata, 'worlddata.csv')
# worlddata = read.csv("worlddata.csv", header =TRUE, stringsAsFactors = FALSE)
# 
# worlddata = worlddata[-1]
# 
# worlddata = gather(worlddata, country, load, Australia:US, factor_key=TRUE)
# worlddata = worlddata[order(worlddata$end), ]
# 
# worlddata

```





```{r 2, echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}

# data = worlddata

# country = c('Australia', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden','US')

# MAKE SARIMA DATA 

# for (country_string in country){print(country_string)
#   # country_string = "US"
# data = worlddata
# data = data %>% filter(country == country_string) 
# data[c("end","load")]
# data
# t20152018 = data[(data$end >= "2015-01-01" & data$end <= "2018-12-31"),]
# t20162019 = data[(data$end >= "2016-01-01" & data$end <= "2019-12-31"),]
# # t20152020 = data
# 
# 
# # Create a daily Date object - helps my work on dates
# inds <- seq(as.Date("2015-01-01"), as.Date("2018-12-31"), by = "day")
# 
# ## Create a time series object
# set.seed(25)
# myts = ts(t20152018$load,
#            start = c(2015, as.numeric(format(inds[1], "%j"))),
#            frequency = 7)
# 
# bestfit <- list(aicc=Inf)
# # Choose the best model by AICc
# for(i in 1:3) {
#   for (j in 1:3){
#     for (k in 1:3){
#     z1 <- fourier(ts(myts, frequency=7), K=i)
#     z2 <- fourier(ts(myts, frequency=365), K=j)
#     z3 <- fourier(ts(myts, frequency=30), K=k)
#     fit <- auto.arima(myts, xreg=cbind(z1, z2, z3), seasonal=F)
#     if(fit$aicc < bestfit$aicc) {
#       bestfit <- list(aicc=fit$aicc, i=i, j=j, k=k, fit=fit)
#     }
#     }
#   }
# }
# 
# # summary(bestfit$fit)
# fcBESTFIT2019 = forecast(bestfit$fit, xreg=cbind(
#                  fourier(ts(myts, frequency=7), K=bestfit$i, h=200),
#                  fourier(ts(myts, frequency=365), K=bestfit$j, h=200),
#                  fourier(ts(myts, frequency=30), K=bestfit$k, h=200)))
# 
# saveRDS(fcBESTFIT2019, file = paste0(country_string,"2019",".rds"))
# fcBESTFIT3 = readRDS(file = "ausdata/bf3.rds")
# Create a daily Date object - helps my work on dates
# inds <- seq(as.Date("2016-01-01"), as.Date("2019-12-31"), by = "day")
# 
# ## Create a time series object
# set.seed(25)
# myts = ts(t20162019$load,
#            start = c(2019, as.numeric(format(inds[1], "%j"))),
#            frequency = 7)
# 
# bestfit <- list(aicc=Inf)
# # Choose the best model by AICc
# for(i in 1:3) {
#   for (j in 1:3){
#     for (k in 1:3){
#     z1 <- fourier(ts(myts, frequency=7), K=i)
#     z2 <- fourier(ts(myts, frequency=365), K=j)
#     z3 <- fourier(ts(myts, frequency=30), K=k)
#     fit <- auto.arima(myts, xreg=cbind(z1, z2, z3), seasonal=F)
#     if(fit$aicc < bestfit$aicc) {
#       bestfit <- list(aicc=fit$aicc, i=i, j=j, k=k, fit=fit)
#     }
#     }
#   }
# }
# 
# # summary(bestfit$fit)
# fcBESTFIT2020 = forecast(bestfit$fit, xreg=cbind(
#                  fourier(ts(myts, frequency=7), K=bestfit$i, h=200),
#                  fourier(ts(myts, frequency=365), K=bestfit$j, h=200),
#                  fourier(ts(myts, frequency=30), K=bestfit$k, h=200)))
# 
# saveRDS(fcBESTFIT2020, file = paste0(country_string,"2020",".rds"))
# }



```




```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}
# worlddata = gather(worlddata, country, load, Australia:US, factor_key=TRUE)
# worlddata = worlddata[order(worlddata$end), ]

country = c('Australia', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden','US')


worlddata = read.csv("worlddata.csv", header =TRUE, stringsAsFactors = FALSE)

worlddata = worlddata[-1]

worlddata = gather(worlddata, country, load, Australia:US, factor_key=TRUE)
worlddata = worlddata[order(worlddata$end), ]


worlddata$end = worlddata$end %>% as.Date()

addsarima = function(model,name){
x = seq(as.Date("2019-01-01"), as.Date("2019-07-19"), by = "day")
coun = rep(name,length(x))
model = data.frame(end = x, SARIMAX2019 = model$mean, country = coun)
model$country = model$country %>% as.factor()
worlddata = left_join(worlddata, model, by=c("end","country"))
return(worlddata)
}

Australia19 = readRDS(file = "SARIMAXdata/Australia2019.rds")
Austria19 = readRDS(file = "SARIMAXdata/Austria2019.rds")
Belgium19 = readRDS(file = "SARIMAXdata/Belgium2019.rds")
Switzerland19 = readRDS(file = "SARIMAXdata/Switzerland2019.rds")
Germany19 = readRDS(file = "SARIMAXdata/Germany2019.rds")
Denmark19 = readRDS(file = "SARIMAXdata/Denmark2019.rds")
Spain19 = readRDS(file = "SARIMAXdata/Spain2019.rds")
France19 = readRDS(file = "SARIMAXdata/France2019.rds")
UK19 = readRDS(file = "SARIMAXdata/UK2019.rds")
Ireland19 = readRDS(file = "SARIMAXdata/Ireland2019.rds")
Italy19 = readRDS(file = "SARIMAXdata/Italy2019.rds")
Luxembourg19 = readRDS(file = "SARIMAXdata/Luxembourg2019.rds")
Netherlands19 = readRDS(file = "SARIMAXdata/Netherlands2019.rds")
Norway19 = readRDS(file = "SARIMAXdata/Norway2019.rds")
Portugal19 = readRDS(file = "SARIMAXdata/Portugal2019.rds")
Sweden19 = readRDS(file = "SARIMAXdata/Sweden2019.rds")
worlddata$SARIMAX2019 = addsarima(Australia19,"Australia")$SARIMAX2019
worlddata = left_join(worlddata, addsarima(Austria19,"Austria"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Belgium19,"Belgium"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Switzerland19,"Switzerland"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Germany19,"Germany"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Denmark19,"Denmark"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Spain19,"Spain"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(France19,"France"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(UK19,"UK"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Ireland19,"Ireland"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Italy19,"Italy"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Luxembourg19,"Luxembourg"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Netherlands19,"Netherlands"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Norway19,"Norway"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Portugal19,"Portugal"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]
worlddata = left_join(worlddata, addsarima(Sweden19,"Sweden"), by=c("end","country","load"))
worlddata$SARIMAX2019 = rowSums(worlddata[,c("SARIMAX2019.x", "SARIMAX2019.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:4)]

worlddata[(worlddata$SARIMAX2019 == 0),]$SARIMAX2019 <- NA


addsarima = function(model,name){
x = seq(as.Date("2020-01-01"), as.Date("2020-07-18"), by = "day")
coun = rep(name,length(x))
model = data.frame(end = x, SARIMAX2020 = model$mean, country = coun)
model$country = model$country %>% as.factor()
worlddata = left_join(worlddata, model, by=c("end","country"))
return(worlddata)
}

Australia20 = readRDS(file = "SARIMAXdata/Australia2020.rds")
Austria20 = readRDS(file = "SARIMAXdata/Austria2020.rds")
Belgium20 = readRDS(file = "SARIMAXdata/Belgium2020.rds")
Switzerland20 = readRDS(file = "SARIMAXdata/Switzerland2020.rds")
Germany20 = readRDS(file = "SARIMAXdata/Germany2020.rds")
Denmark20 = readRDS(file = "SARIMAXdata/Denmark2020.rds")
Spain20 = readRDS(file = "SARIMAXdata/Spain2020.rds")
France20 = readRDS(file = "SARIMAXdata/France2020.rds")
UK20 = readRDS(file = "SARIMAXdata/UK2020.rds")
Ireland20 = readRDS(file = "SARIMAXdata/Ireland2020.rds")
Italy20 = readRDS(file = "SARIMAXdata/Italy2020.rds")
Luxembourg20 = readRDS(file = "SARIMAXdata/Luxembourg2020.rds")
Netherlands20 = readRDS(file = "SARIMAXdata/Netherlands2020.rds")
Norway20 = readRDS(file = "SARIMAXdata/Norway2020.rds")
Portugal20 = readRDS(file = "SARIMAXdata/Portugal2020.rds")
Sweden20 = readRDS(file = "SARIMAXdata/Sweden2020.rds")
worlddata$SARIMAX2020 = addsarima(Australia20,"Australia")$SARIMAX2020
worlddata = left_join(worlddata, addsarima(Austria20,"Austria"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Belgium20,"Belgium"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Switzerland20,"Switzerland"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Germany20,"Germany"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Denmark20,"Denmark"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Spain20,"Spain"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(France20,"France"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(UK20,"UK"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Ireland20,"Ireland"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Italy20,"Italy"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Luxembourg20,"Luxembourg"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Netherlands20,"Netherlands"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Norway20,"Norway"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Portugal20,"Portugal"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]
worlddata = left_join(worlddata, addsarima(Sweden20,"Sweden"), by=c("end","country","load","SARIMAX2019"))
worlddata$SARIMAX2020 = rowSums(worlddata[,c("SARIMAX2020.x", "SARIMAX2020.y")], na.rm=TRUE)
worlddata = worlddata[,c(1:5)]

worlddata[(worlddata$SARIMAX2020 == 0),]$SARIMAX2020 <- NA

# worlddata[(worlddata$end >= "2020-01-01"),]

```



```{r, echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}

country = c('Australia', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden','US')


button_list <- lapply(1:length(country), function(x){
  list(method = "restyle",
       args = list("transforms[0].value", country[x]),
       label = country[x])
})



p <- plot_ly(data = worlddata, x = ~end, y = ~load, type = 'scatter', text = ~end, hoverinfo =~as.factor(end),
             mode = 'lines', alpha = 1, line = list(color = '#06d6a0', width = 2), name = paste0(unique(worlddata$country)[1]),
             transforms = list( list(
                            type = 'filter',
                            target = ~country,
                            operation = '=',
                            value = unique(worlddata$country)[1]
      ))) %>% layout(
        title = paste0('SARIMAX on Energy Consumption'),
         xaxis = list(title = 'Date'),
         yaxis = list(title = 'MW of Energy'),
    updatemenus = list(
      list(
  xanchor = 'left',
  yanchor = "top",
  pad = list('r'= 0, 't'= 10, 'b' = 10),
  x = 0,
  y = 1.27,
        type = 'dropdown',
        active = 0,
        buttons = button_list
        )
      )
    )
  
# p <- plotly_build(p)


p = p %>% layout(xaxis = list(
        range =
            c(as.numeric(as.POSIXct("2018-11-01", format="%Y-%m-%d"))*1000,
              as.numeric(as.POSIXct("2020-06-30", format="%Y-%m-%d"))*1000),
        type = "date")) 

p = p  %>% layout(xaxis = list(rangeslider = list(type = "date")))

p = p  %>% add_trace(data = worlddata, x = ~end, y = ~SARIMAX2019, type = 'scatter',line=list(color='#e63946',dash='dashed'),
             mode = 'lines', name = 'SARIMAX 2019', alpha = 1)
p = p  %>% add_trace(data = worlddata, x = ~end, y = ~SARIMAX2020, type = 'scatter',line=list(color='#457b9d',dash='dashed'),
             mode = 'lines', name = 'SARIMAX 2020', alpha = 1)
p

# saveRDS(p, file = "worldtime.rds")
# worldtime = readRDS(file = "worldtime.rds")

```

The figure above is a time-series of electricity consumption in each country. with an overlay of a SARIMAX model predicting energy consumption in 2019 vs 2020, having been trained on the previous 4 years worth of data. The appendix outlines the accuracy of these models.The objective of this plot and table is to illustrate the predictability of electricity consumption data during a normal year vs a pandemic enduced year. The change is clearly picked up in the drastic changes in accuracy.

Electricity consumption can be attributed to weather, seasonality, business cycles & base load. Weather and seasonality are in present moment indicators. The temperature causes one to use heating and cooling measures. Seasonality will effect the duration of lighting usage. Business cycles and base load (level of business activity) can be presumptiously reactive to conditions unlike weather and seasonality which are reactive in the moment. Hence, electricity consumption attributed to business cycles and base load can be a forward looking indicator into economic activity. When diving into the plots individually of each country, it is clear that electricity consumption is reactive to policy changes. Using France as an example, their full lockdown began 12 weeks into 2020 and ended in week 20. The 2020 forecast is almost perfectly out of place between week 12 and 20, before lining up again with the actual consumption.

```{r fig.cap="Figure 1: Descriptive Plots" , echo=FALSE, fig.align='left', fig.height=9, fig.width=26, results='hide'}

# fr = read.csv("eurodata/fr.csv", header =TRUE, stringsAsFactors = FALSE)
# fr$end = fr$end %>% as.Date()
# fr = aggregate(fr["load"], by=fr["end"], sum)
# fr = fr[1:(length(fr$load)-1),]
# fr = fr[(fr$end> "2018-01-01	" & fr$end < "2020-07-30"),]
# df = fr
# setwd("eurodata/")
# temp = list.files(pattern="*.csv")
# for (i in 1:length(temp)){
#   fr = read.csv(temp[i], header = TRUE, stringsAsFactors = FALSE)
#   fr = na.omit(fr)
#   fr$end = fr$end %>% as.Date()
#   fr = aggregate(fr["load"], by=fr["end"], sum)
#   fr = fr[1:(length(fr$load)-1),]
#   fr = fr[(fr$end> "2018-01-01	" & fr$end < "2020-07-30"),]
#   fr[,paste0(temp[i])] = fr$load
#   df = left_join(df, fr, by=c("end"))
# }
# df = df[, -grep("load", colnames(df))]
# colnames(df) = c('end', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden')
# 
# dfwide = gather(df, country, load, Austria:Sweden, factor_key=TRUE)
# 
# write.csv(dfwide, 'widedata.csv')
# dfwide = read.csv("widedata.csv", header =TRUE, stringsAsFactors = FALSE)
# dfwide = dfwide[-1]
```



```{r fig.cap="Figure 1: Descriptive Plots" , echo=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}

# p <- plot_ly() %>%
#   add_trace( data = dfwide, x = ~end, y = ~load, type = 'scatter', 
#              mode = 'lines', color = ~as.factor(country))
# p <- plotly_build(p)
# 
# legendItems <- list(Austria = 'legendonly', Belgium= 'legendonly', Switzerland= 'legendonly', Germany= 'legendonly', Denmark= 'legendonly', Spain= 'legendonly', France= TRUE, UK= 'legendonly', Ireland= 'legendonly', Italy= TRUE, Luxembourg= 'legendonly', Netherlands= 'legendonly', Norway= 'legendonly', Portugal= 'legendonly', Sweden= 'legendonly')
# 
# for(i in seq_along(p$x$data)){
#   p$x$data[[i]]$visible <- legendItems[[p$x$data[[i]]$name]]
# }
# 
# #### LONG FORMAT DATA 
# # fig = plot_ly(df, x = ~end, y = ~Spain, name = "Spain", type = 'scatter', mode = 'lines', colors = "red") 
# # xy.list <- as.list(as.data.frame((df)))
# # for(i in colnames(df)[-1]) { fig <- add_trace(fig, y=xy.list[[i]], x=xy.list[['end']], 
# #                                               name = paste0(i),  type = 'scatter', 
# #                                               mode = 'lines',evaluate = TRUE) }
# 
# p = p %>% layout(title = 'Energy Consumption in Western Europe',
#          xaxis = list(title = 'Time'),
#          yaxis = list(title = 'Energy Consumption (MW)'))
# 
# p = p %>%
#     layout(xaxis = list(
#         range = 
#             c(as.numeric(as.POSIXct("2018-00-01", format="%Y-%m-%d"))*1000,
#               as.numeric(as.POSIXct("2020-07-30", format="%Y-%m-%d"))*1000),
#         type = "date"))
# 
# p

```




```{r fig.cap="Figure 2: Descriptive Plots" , echo=FALSE, fig.align='left', fig.height=9, fig.width=26, results='show'}
# fr = read.csv("eurodata/fr.csv", header =TRUE, stringsAsFactors = FALSE)
# fr = na.omit(fr)
# fr$end = fr$end %>% as.Date()
# fr = aggregate(fr["load"], by=fr["end"], sum)
# fr = fr[1:(length(fr$load)-1),]
# TRAIN2015TO2018 = fr[(fr$end> "2015-01-01	" & fr$end < "2018-12-31"),]
# TEST2017 = fr[(fr$end> "2017-01-01" & fr$end < "2017-07-30"),]
# TEST2018 = fr[(fr$end> "2018-01-01" & fr$end < "2018-07-30"),]
# TEST2019 = fr[(fr$end> "2019-01-01" & fr$end < "2019-07-30"),]
# TRAIN2016TO2019 = fr[(fr$end> "2016-01-01	" & fr$end < "2019-12-31"),]
# TEST2020 = fr[(fr$end> "2020-01-01" & fr$end < "2020-07-30"),]
# 
# x1 = ggplot(fr, aes(x=end, y=load, color = "Electricity Consumed")) +
#   geom_line(size = 2) + theme_minimal() +
#   xlab("Date") + ylab("MW of Electricity Consumed") + scale_x_date(date_labels = "%Y") + ggtitle("FRANCE ELECTRICITY CONSUMPTION")
# 
# fcBESTFIT1 = readRDS(file = "eurodata/bf1.rds")
# fcBESTFIT2 = readRDS(file = "eurodata/bf2.rds")
# p = readRDS(file = "eurodata/p.rds")
# q = readRDS(file = "eurodata/q.rds")

# lay <- rbind(c(1,1,1,1),
#              c(2,2,3,3))
# 
# grid.arrange(x1, p, q, layout_matrix = lay)

# a <- list(
#   text = "TRAINING ON 2015 TO 2018 AND TESTING ON THE FIRST 209 DAYS OF 2019 - FRANCE ELECTRICITY CONSUMPTION",
#   xref = "paper",
#   yref = "paper",
#   yanchor = "bottom",
#   xanchor = "center",
#   align = "center",
#   x = 0.5,
#   y = 1,
#   showarrow = FALSE
# )
# 
# b <- list(
#   text = "TRAINING ON 2016 TO 2019 AND TESTING ON THE FIRST 209 DAYS OF 2020 - FRANCE ELECTRICITY CONSUMPTION
# ",
#   xref = "paper",
#   yref = "paper",
#   yanchor = "bottom",
#   xanchor = "center",
#   align = "center",
#   x = 0.5,
#   y = 1,
#   showarrow = FALSE
# )
# 
# 
# subplot(ggplotly(p)%>%layout(annotations = a), 
#         ggplotly(q)%>%layout(annotations = b), shareY = TRUE)

```


<!-- The top plot in figure 1. is a time-series of electricity consumption in France. The bottom two plots show a SARIMAX model predicting energy consumption in 2019 vs 2020, having been trained on the previous 4 years worth of data. Table 1 shows the accuracy of this model on various European countries. The methodology behind this is explained in Appendix 4.1.  -->

<!-- The objective of this plot and table is to illustrate the predictability of electricity consumption data during a normal year vs a pandemic enduced year. The change is clearly picked up in the drastic changes in accuracy shown in table 1. Electricity consumption can be attributed to weather, seasonality, business cycles & base load. Weather and seasonality are in present moment indicators. The temperature causes one to use heating and cooling measures. Seasonality will effect the duration of lighting usage. Business cycles and base load (level of business activity) can be presumptiously reactive to conditions unlike weather and seasonality which are reactive in the moment. Hence, electricity consumption attributed to business cycles and base load can be a forward looking indicator into economic activity. -->

<!-- When diving into the plots individually of each country, it is clear that electricity consumption is reactive to policy changes. Using France as an example, their full lockdown began 12 weeks into 2020 and ended in week 20. The bottom right plot in figure 1 shows the forecast is almost perfectly out of place between week 12 and 20, before lining up again with the actual consumption.  -->

<!-- \begin{table}[!htbp] \centering  -->
<!--   \caption{Accuracy of SARIMAX on various countries} -->
<!--   \label{tab:my-table} -->
<!-- \begin{tabular}{llllll} -->
<!-- \\[-1.8ex]\hline  -->
<!-- \hline \\[-1.8ex]  -->
<!--  & ME & RMSE & MAE & MPE & MAPE \\ -->
<!-- \hline \\[-1.8ex]  -->
<!-- FRANCE TEST 2019 & -2925.663 & 90265.63 & 70169.87 & -0.4553922 & 5.26192 \\ -->
<!-- FRANCE TEST 2020 & -107436.7 & 143925.8 & 114499.1 & -9.782641 & 10.21085 \\ -->
<!-- GERMANY TEST 2019 & 610927.5 & 672348.8 & 649246.9 & 11.0203 & 11.92293 \\ -->
<!-- GERMANY TEST 2020 & -264620.9 & 407832.7 & 318019.9 & -5.713203 & 6.627528 \\ -->
<!-- SPAIN TEST 2019 & 6880.948 & 49946.31 & 40349.3 & 0.4713497 & 5.799492 \\ -->
<!-- SPAIN TEST 2020 & -50412.72 & 68669.14 & 58031.49 & -8.72553 & 9.714945 \\ -->
<!-- UNITED KINGDOM 2019 & 7891.58 & 90497.34 & 62057.25 & 0.07401255 & 4.054127 \\ -->
<!-- UNITED KINGDOM 2020 & -128479.9 & 172001.5 & 138371.7 & -9.760755 & 10.28156 \\ -->
<!-- SWEDEN 2019 & -580.2107 & 26802.2 & 21601.13 & -0.7898409 & 5.681146 \\ -->
<!-- SWEDEN 2020 & -16267.63 & 27859.37 & 21362.1 & -4.376811 & 5.734472 \\ -->
<!-- ITALY 2019 &  90675.5 & 105149.6 & 95438.57 & 10.63521 & 11.47715 \\ -->
<!-- ITALY 2020 &  91497.45 & 136313.6 & 115616.3 & 11.01898 & 15.24938 \\ -->
<!-- \hline \\[-1.8ex]  -->
<!-- \end{tabular} -->
<!-- \end{table} -->



<center> <h2>**Demand & Economics**: Duck curve & the economy - an analysis into electricity consumption</h2> </center>

With this rise in solar energy production anticipation of electricity demand has been on the forefront of utility companies. The duck curve simply is the demand for electricity any given time during the day. The early mornings consist of low energy demands, but as people wake up and busineses begin production the demand rises. Then peaks around sunset before dropping. As you may have guessed the production of energy during the peak hours of the day helps reduce the demand needed. This is evident as the year go on, the dip during mid day drops lower and lower forming a duck like shape. This is more prominent in countries with renewable energy rebates such as Sewden's tax regulation mechanisms and a subsidy scheme. 

There's two critical issues that rise from this. One, as steeper drops in demand occur more rapid increases in the production are needed during peak hours with no sunlight - this poses a serious burden on power infratructure and diminish efficency. 
The second issue is that during days of over production in solar energy, grid managers turn off solar panels to prevent overloading the power grid and throw away extra solar energy. 

The solution is as battery technology evolves, we're able to store energy more efficiently and locally reducing waste while moving away from non-renewables. 












```{r, echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



# WESTERN EURO DATA HOUR

fr = read.csv("eurodata/fr.csv", header =TRUE, stringsAsFactors = FALSE)
fr = na.omit(fr)
fr$hour = substr(fr$end,11,nchar(fr$end)-12)
fr$year = substr(fr$end,1,nchar(fr$end)-21)
fr  = aggregate(load ~ hour + year, data = fr, mean)
df = fr
setwd("eurodata/")
temp = list.files(pattern="*.csv")
for (i in 1:length(temp)){
  fr = read.csv(temp[i], header =TRUE, stringsAsFactors = FALSE)
  fr = na.omit(fr)
  fr$hour = substr(fr$end,11,nchar(fr$end)-12)
  fr$year = substr(fr$end,1,nchar(fr$end)-21)
  fr  = aggregate(load ~ hour + year, data = fr, mean)
  fr[,paste0(temp[i])] = fr$load
  df = left_join(df, fr, by=c("hour","year"))
}
df = df[, -grep("load", colnames(df))]
colnames(df) = c('hour','year', 'Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden')

eurohourwide = gather(df, country, load, Austria:Sweden, factor_key=TRUE)
eurohourwide = eurohourwide[-3]

country = c('Austria', 'Belgium', 'Switzerland', 'Germany', 'Denmark', 'Spain', 'France', 'UK', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal','Sweden')

eurohourwide <- eurohourwide[order(eurohourwide$year), ]



# build up interactive element, the filter to the two groups
button_list <- lapply(1:length(country), function(x){
  list(method = "restyle",
       args = list("transforms[0].value", country[x]),
       label = country[x])
})

# fr = read.csv("fr.csv", header =TRUE, stringsAsFactors = FALSE)
# fr = na.omit(fr)
# fr$hour = substr(fr$end,11,nchar(fr$end)-12)
# fr$year = substr(fr$end,1,nchar(fr$end)-21)
# fr  = aggregate(load ~ hour + year, data = fr, mean

p <- plot_ly(data = eurohourwide, x = ~hour, y = ~load, type = 'scatter', text = ~year, hoverinfo =~as.factor(year),
             mode = 'lines', 
             color = ~as.factor(year), alpha = 0.7,
             transforms = list( list(
                            type = 'filter',
                            target = ~country,
                            operation = '=',
                            value = unique(eurohourwide$country)[1]
      ))) %>% layout(
        title = paste0('Intraday Energy Consumption'),
         xaxis = list(title = 'Hour of Day'),
         yaxis = list(title = 'MW of Energy'),
    updatemenus = list(
      list(
  xanchor = 'left',
  yanchor = "top",
  pad = list('r'= 0, 't'= 10, 'b' = 10),
  x = 0,
  y = 1.27,
        type = 'dropdown',
        active = 0,
        buttons = button_list
        )
      )
    )
  
p <- plotly_build(p)

# p = p %>% layout(title = 'Intraday Energy Consumption',
#          xaxis = list(title = 'Time'),
#          yaxis = list(title = 'MW of Energy'))
p 



```



<!-- <div style="text-align: center;"><iframe width="560" height="315" src="https://www.youtube.com/embed/YYLzss58CLs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div> -->


<center> <h2>**Anomalous Behaviours**: An experimental approach to detect change in production activity</h2> </center>



A quick look into anomalous number patterns in 2020 electricty load data in Western Europe.

This takes a look at the first digit of the sum of electricity load of every hour in per country in western europe. In combination with *Law of large numbers*, Benfords Law should be able to tell us if there was a signficant change in 2020. 



NOTE TO SELF: not enough data in hour to adhere to law of large numbers, need to trytransforming data into 15min periods to encapsulate this maybe? 


<center> <h3>Benfords law on Electricity Load 2020 VS 2019 In Western Europe</h3> </center>
```{r, echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}

library(benford.analysis)
library(gridExtra)


euro2020 = eurohourwide %>% filter(year == 2020) 
euro2019 = eurohourwide %>% filter(year == 2019) 

# eurodiff$load = euro2020$load - euro2019$load

trends2020 = benford(euro2020$load, number.of.digits = 1, discrete = T, sign = "positive") 
trends2019 = benford(euro2019$load, number.of.digits = 1, discrete = T, sign = "positive") 
# trendsdiff = benford(eurodiff$load, number.of.digits = 1, discrete = T, sign = "positive") 


trends2020$data = trends2020$data %>% mutate(bl=recode(data.digits, 
                         `1`=30.1,`2`= 17.6,`3`= 12.5,`4`= 9.7,`5`= 7.9,`6`= 6.7,`7`= 5.8,`8`= 5.1,`9`= 4.6,))

trends2019$data = trends2019$data %>% mutate(bl=recode(data.digits, 
                         `1`=30.1,`2`= 17.6,`3`= 12.5,`4`= 9.7,`5`= 7.9,`6`= 6.7,`7`= 5.8,`8`= 5.1,`9`= 4.6,))

trends2020$s.o.data = trends2020$s.o.data %>% mutate(bl=recode(data.second.order.digits, 
                         `1`=30.1,`2`= 17.6,`3`= 12.5,`4`= 9.7,`5`= 7.9,`6`= 6.7,`7`= 5.8,`8`= 5.1,`9`= 4.6,))
trends2019$s.o.data = trends2019$s.o.data %>% mutate(bl=recode(data.second.order.digits, 
                         `1`=30.1,`2`= 17.6,`3`= 12.5,`4`= 9.7,`5`= 7.9,`6`= 6.7,`7`= 5.8,`8`= 5.1,`9`= 4.6,))

# 
# ggdiff1 = ggplot(trendsdiff$data,aes(x = data.digits, fill = data.digits)) + 
#   geom_bar(position = "identity", fill = "#63FF83") + 
#   theme_minimal() + xlab("First Digit")
# 
# 
# ggdiff2 = ggplot(trendsdiff$s.o.data,aes(x = data.second.order.digits, fill = data.second.order.digits)) + 
#   geom_bar(position = "identity", fill = "#63FF83") + 
#   theme_minimal() + xlab("Second Order First Digit")


gg1 = ggplot(trends2020$data,aes(x = data.digits, fill = data.digits)) + 
  geom_bar(position = "identity", fill = "#90e0ef") + 
  theme_minimal() + xlab("First Digit")+ geom_line(aes(x = data.digits, y = bl*3),inherit.aes = FALSE, size = 1, color="black", alpha = 0.5, group = 1) + 
  scale_y_continuous(sec.axis = sec_axis(~. /3))
# + labs(title="Benfords Law")


gg2 = ggplot(trends2020$s.o.data,aes(x = data.second.order.digits, fill = data.second.order.digits)) + 
  geom_bar(position = "identity", fill = "#90e0ef") + 
  theme_minimal() + xlab("Second Order First Digit") + geom_line(aes(x = data.second.order.digits, y = bl*3),inherit.aes = FALSE, size = 1, color="black", alpha = 0.5, group = 1) + 
  scale_y_continuous(sec.axis = sec_axis(~. /3))#+ labs(title="Second Order Benfords Law")



gg3 = ggplot(trends2019$data,aes(x = data.digits, fill = data.digits)) + 
  geom_bar(position = "identity", fill = "#ade8f4") + 
  theme_minimal() + xlab("First Digit")+ geom_line(aes(x = data.digits, y = bl*3),inherit.aes = FALSE, size = 1, color="black", alpha = 0.5, group = 1) + 
  scale_y_continuous(sec.axis = sec_axis(~. /3))


gg4 = ggplot(trends2019$s.o.data,aes(x = data.second.order.digits, fill = data.second.order.digits)) + 
  geom_bar(position = "identity", fill = "#ade8f4") + 
  theme_minimal() + xlab("Second Order First Digit")+ geom_line(aes(x = data.second.order.digits, y = bl*3),inherit.aes = FALSE, size = 1, color="black", alpha = 0.5, group = 1) + 
  scale_y_continuous(sec.axis = sec_axis(~. /3))



a <- list(
  text = "2020 First Order Benfords Law",
  xref = "paper",
  yref = "paper",
  yanchor = "bottom",
  xanchor = "center",
  align = "center",
  x = 0.5,
  y = 1,
  showarrow = FALSE
)

b <- list(
  text = "2020 Second Order Benfords Law",
  xref = "paper",
  yref = "paper",
  yanchor = "bottom",
  xanchor = "center",
  align = "center",
  x = 0.5,
  y = 1,
  showarrow = FALSE
)

c <- list(
  text = "2019 First Order Benfords Law",
  xref = "paper",
  yref = "paper",
  yanchor = "bottom",
  xanchor = "center",
  align = "center",
  x = 0.5,
  y = 1,
  showarrow = FALSE
)

d <- list(
  text = "2019 Second Order Benfords Law",
  xref = "paper",
  yref = "paper",
  yanchor = "bottom",
  xanchor = "center",
  align = "center",
  x = 0.5,
  y = 1,
  showarrow = FALSE
)
ay <- list(
  overlaying = "y",
  side = "right",
  title = "Ideal Benfords Law"
)


subplot(ggplotly(gg1)%>%layout(annotations = a),
        ggplotly(gg2)%>%layout(annotations = b), 
        ggplotly(gg3)%>%layout(annotations = c),
        ggplotly(gg4)%>%layout(annotations = d), 
        nrows = 2,shareY = TRUE)%>% layout(showlegend = F, yaxis = list(anchor = 'x1'), 
            yaxis2 = list(showticklabels = T), yaxis4 = list(showticklabels = F), 
            xaxis = list(showticklabels = F), xaxis2 = list(showticklabels = F)) %>%
    add_annotations(
        text = "First Digit in Hourly Load",
        x = 0.5,
        y = 0,
        yref = "paper",
        xref = "paper",
        xanchor = "center",
        yanchor = "bottom",
        yshift = -35,
        showarrow = FALSE,
        font = list(size = 15)
    ) 
#%>% layout(title= "Benfords Law")

# ggplotly(gg)



# ggplotly(plot(trends))

# trends2020$data = trends2020$data %>% mutate(bl=recode(data.digits, 
#                          `1`=30.1,`2`= 17.6,`3`= 12.5,`4`= 9.7,`5`= 7.9,`6`= 6.7,`7`= 5.8,`8`= 5.1,`9`= 4.6,))
# 
# # euro2020 = eurohourwide %>% filter(year == 2020) %>% select(load)
# gg1 + geom_line(aes(x = data.digits, y = bl*3),inherit.aes = FALSE, size = 1.5, color="black", alpha = 0.5, group = 1) + 
#   scale_y_continuous(sec.axis = sec_axis(~. /3))

```


<div style="text-align: center;"><iframe width="560" height="315" src="https://www.youtube.com/embed/XXjlR2OK1kM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>


```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```




```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```



```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```


```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```




```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```




```{r , echo=FALSE, cache=FALSE, fig.align='left', fig.height=4.5, fig.width=9, results='show'}



```














